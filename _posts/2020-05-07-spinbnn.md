---
title: 'All Spin Bayesian Neural Networks'
date: 2020-05-07
permalink: /posts/2020/05/spinbnn/
tags:
  - bayesian neural network
  - spintronic
  - hardware acceleration
---
**Briefly explain the problem you worked on.**
The problem I worked on is the hardware acceleration of Bayesian neural networks using spintronic devices. 
Hardware acceleration is the design of special-purpose hardware so that certain functions can run more efficiently (lower power consumption, faster execution, etc) than they would run on a general-purpose CPU. A common example of hardware acceleration is the graphical processing unit (GPU), which performs various complex mathematical and geometrical operations for graphics rendering more efficiently than a general-purpose CPU.    

Bayesian neural networks (BNN) are structurally similar to traditional neural networks. The fundamental difference between the two is that the weights of a BNN are probability distributions rather than fixed-point values. These distributions are generally modeled as Gaussian distributions, each having its own mean and variance. During the training stage, the best set of means and variances are obtained for each weight, so that the network can make predictions with maximum accuracy. Figure 1 shows the forward propagation of input data through the network. When data is inputted into the network for prediction, the weights used are sampled from their respective probability distributions. Therefore, if this process is repeated multiple times, the final output of the network can be characterized as a probability distribution, with a certain mean and variance, rather than just a fixed point value.
![](https://akulmalhotra.github.io/files/spinbnn/network.jpg)


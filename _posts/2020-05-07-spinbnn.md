---
title: 'All Spin Bayesian Neural Networks'
date: 2020-05-07
permalink: /posts/2020/05/spinbnn/
tags:
  - bayesian neural network
  - spintronic
  - hardware acceleration
---
**Briefly explain the problem you worked on.**
<p style='text-align: justify;'>
The problem I worked on is the hardware acceleration of Bayesian neural networks using spintronic devices. 
Hardware acceleration is the design of special-purpose hardware so that certain functions can run more efficiently (lower power consumption, faster execution, etc) than they would run on a general-purpose CPU. A common example of hardware acceleration is the graphical processing unit (GPU), which performs various complex mathematical and geometrical operations for graphics rendering more efficiently than a general-purpose CPU. </p>

<p style='text-align: justify;'>
Bayesian neural networks (BNN) are structurally similar to traditional neural networks. The fundamental difference between the two is that the weights of a BNN are probability distributions rather than fixed-point values. These distributions are generally modeled as Gaussian distributions, each having its own mean and variance. During the training stage, the best set of means and variances are obtained for each weight, so that the network can make predictions with maximum accuracy. Figure 1 shows the forward propagation of input data through the network. When data is inputted into the network for prediction, the weights used are sampled from their respective probability distributions. Therefore, if this process is repeated multiple times, the final output of the network can be characterized as a probability distribution, with a certain mean and variance, rather than just a fixed point value. </p>

<p align="center">
  <img src="https://akulmalhotra.github.io/files/spinbnn/network.jpg?raw=true" alt="Photo" style="width: 450px;"/> 
</p>

<p style='text-align: center;'>
Figure 1: Forward propagation of data through a BNN </p>

Spintronic devices are an emerging technology that utilizes both the electric charge and the spin of the electron for logic and memory applications. The basic device structure used in this work is the Magnetic tunnel junction (MTJ), which is a two-terminal device consisting of two nanomagnets sandwiching a metal oxide layer. I’m not going to go into the details of the device structure, but here are some fundamental things to know. Firstly, the resistance of the device depends on the relative spin of the electrons of the two nanomagnets: if the spins are in the same (opposite) direction, the device is said to be in the parallel (antiparallel) state and possesses its lowest (highest) possible resistance. Secondly, since the state of the device is stored in its spin, it is able to retain its state without any external power supply (unlike an SRAM cell, which stores the bit only when power is on). Thirdly, the device can be used either as a two state (parallel and antiparallel) device with just two possible resistance values or a multi-state (many states between parallel and antiparallel) device with multiple possible resistance values. I’ll talk more about how to switch between the states of an MTJ later.       
 
One thing to keep in mind is that the hardware I designed was for a BNN which has been trained offline. Offline training means that the network has been pre-trained on a certain dataset on the CPU (by running the algorithm on a deep learning framework like Pytorch) and the learnt means and variances of the weights have been stored onto the hardware platform. The system is then used to make future predictions on input data. 
   
**Why is solving this problem important/ How is solving this problem beneficial?**

With artificial intelligence becoming more and more ubiquitous, neural networks are being applied to almost every domain. This includes sensitive areas such as autonomous vehicles, healthcare, and security. In many real-time applications, there is no scope for incorrect decisions and not making a prediction is better than making the wrong one. A traditional neural network provides a prediction based on the input data but will do the same for data it isn’t trained for (image of a leopard in a cat vs. dog classifier) with high confidence. A BNN aims to solve this problem by not only providing the result but also providing how uncertain it is about the result. This is the case because the output is no longer a fixed point value, but a probability distribution. The variance of the output’s distribution can be used as the uncertainty measure to assess how confident the network is of its prediction.

**What are the current state of the art techniques to solve this problem?**

The fundamental hardware design requirements for a BNN are as follows:
- A Gaussian random number generator (GRNG): In order to sample the weights from their respective Gaussian distributions, a Gaussian random generator would be essential to the design.
- Multiplication between inputs and weights, followed by addition: This is the most fundamental operation for the forward propagation of information.
- Activation function: The hardware implementation of the rectified linear (ReLU) / sigmoid/ tanh neuron. 

[[1]](https://dl.acm.org/doi/10.1145/3296957.3173212) is a state of the art CMOS based (using only MOSFETS) BNN hardware acceleration scheme. It uses a linear feedback register to create a binomial distribution and then uses the binomial distribution approximation method to obtain a Gaussian random number. It has a memory unit that stores the learnt means and variances of the weights from offline training. A separate multiplication and addition unit and ReLU neuron unit is used for the forward propagation of input data.
